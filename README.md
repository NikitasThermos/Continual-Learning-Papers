# Continual Learning Papers


## Table of Contents
1. [Catastrophic Forgetting](#cf)
2. [Stability vs Plasticity](#svp)
3. [Continual Learning Evaluation](#eval)
4. [Continual Learning Surveys/Reviews](#surveys)
5. [Continual Learning Applications](#apps)
6. [Continual Learning Frameworks](#frameworks)
7. [Parameter Isolation Methods](#pi)

<a name='cf'></a> 
## Catastrophic Forgetting 
* [Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem](https://www.sciencedirect.com/science/article/abs/pii/S0079742108605368) by Michael McCloskey, Neal J. Cohen. Psychology of Learning and Motivation Vol.24 (1989)
* [Virtual memories and Massive Generalization in Connectionist Combinatorial Learning](https://escholarship.org/uc/item/0dn8w2wb) by Brousse, O., & Smolensky, P. In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 11)(1989).
* [Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions](https://psycnet.apa.org/buy/1990-18992-001) by Ratcliff, Roger. Psychological Review, Vol 97(2)(1990)
* [Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionlst Networks](https://cdn.aaai.org/Symposia/Spring/1993/SS-93-06/SS93-06-007.pdf) by French, R. M. In Proceedings of the 13th annual cognitive science society conference (Vol. 1, pp. 173-178)(1991).
* [Semi-distributed Representations and Catastrophic Forgetting in Connectionist Networks](https://www.tandfonline.com/doi/abs/10.1080/09540099208946624) by French, R. M. Connection Science, 4(3-4)(1992)
* [Catastrophic Interference is Eliminated in Pretrained Networks](https://www.researchgate.net/profile/Ken-Mcrae/publication/2418146_Catastrophic_Interference_is_Eliminated_in_Pretrained_Networks/links/0f3175322732b75ca9000000/Catastrophic-Interference-is-Eliminated-in-Pretrained-Networks.pdf) by McRae, K., & Hetherington, P. A.  In Proceedings of the 15h annual conference of the cognitive science society (Vol. 1, p. 2)(1993).
* [Catastrophic forgetting in connectionist networks](https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2?ref=https%3A%2F%2Fgithubhelp.com) by Robert M. French. Trends in cognitive sciences, 3(4)(1999)
* [An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks](https://arxiv.org/abs/1312.6211) by Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., & Bengio, Y. arXiv preprint(2013)
* [Measuring Catastrophic Forgetting in Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/11651) by Kemker, R., McClure, M., Abitino, A., Hayes, T., & Kanan, C. In Proceedings of the AAAI conference on artificial intelligence (Vol. 32, No. 1)(2018).
* [An Empirical Study of Example Forgetting during Deep Neural Network Learning](https://arxiv.org/abs/1812.05159) by  Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, Geoffrey J. Gordon. ICLR 2019
* [Localizing Catastrophic Forgetting in Neural Networks](https://arxiv.org/abs/1906.02568) by Wiewel, F., & Yang, B. arXiv preprint(2019)
* [Toward Understanding Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/1908.01091) by Nguyen, C. V., Achille, A., Lam, M., Hassner, T., Mahadevan, V., & Soatto, S.  arXiv preprint(2019). 
* [Sequential Mastery of Multiple Visual Tasks: Networks Naturally Learn to Learn and Forget to Forget](https://openaccess.thecvf.com/content_CVPR_2020/html/Davidson_Sequential_Mastery_of_Multiple_Visual_Tasks_Networks_Naturally_Learn_to_CVPR_2020_paper.html) by Guy Davidson, Michael C. Mozer.  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020
* [Understanding the Role of Training Regimes in Continual Learning](https://proceedings.neurips.cc/paper/2020/hash/518a38cc9a0173d0b2dc088166981cf8-Abstract.html?ref=https://githubhelp.com) by Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, Hassan Ghasemzadeh. Advances in Neural Information Processing Systems, 33(2020) 
* [Continual Learning in Deep Networks: an Analysis of the Last Layer](https://arxiv.org/abs/2106.01834) by Lesort, T., George, T., & Rish, I. arXiv preprint(2021)
* [Architecture Matters in Continual Learning](https://arxiv.org/abs/2202.00275) by Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Timothy Nguyen, Razvan Pascanu, Dilan Gorur, Mehrdad Farajtabar. arXiv preprint (2022)
* [Wide Neural Networks Forget Less Catastrophically](https://proceedings.mlr.press/v162/mirzadeh22a.html) by Mirzadeh, S. I., Chaudhry, A., Yin, D., Hu, H., Pascanu, R., Gorur, D., & Farajtabar, M.  In International Conference on Machine Learning(2022)  

<a name='svp'></a>
## Stability vs Plasticity
* [Memory retention – the synaptic stability versus plasticity dilemma](https://www.cell.com/trends/neurosciences/abstract/S0166-2236(04)00370-4) by Abraham, W. C., & Robins, A.Trends in neurosciences, 28(2)(2005)
* [The stability-plasticity dilemma: investigating the continuum from catastrophic forgetting to age-limited learning effects](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2013.00504/full) by Mermillod, M., Bugaiska, A., & Bonin, P. . Frontiers in psychology, 4, 504(2013).    

<a name='eval'></a>
## Continual Learning Evaluation
* [Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines](https://arxiv.org/abs/1810.12488) by Hsu, YenChang, YenCheng Liu, Anita Ramasamy, and Zsolt Kira.Continual Learning Workshop. In 32nd Conference on Neural Information Processing Systems.(2018)
* [Towards Robust Evaluations of Continual Learning](https://arxiv.org/abs/1805.09733) by Farquhar, S., & Gal, Y. arXiv preprint(2018)  
* [Three scenarios for continual learning](https://arxiv.org/abs/1904.07734) by Van de Ven, G. M., & Tolias, A. S. arXiv preprint(2019)

<a name='surveys'></a>
## Continual Learning Surveys/Reviews
* [Continual lifelong learning with neural networks: A review](https://www.sciencedirect.com/science/article/pii/S0893608019300231) by Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., & Wermter, S. Neural networks, 113 (2019)
* [Continual Learning with Neural Networks: A Review](https://dl.acm.org/doi/abs/10.1145/3297001.3297062) by Awasthi, A., & Sarawagi, S.  In Proceedings of the ACM India Joint International Conference on Data Science and Management of Data (2019)
* [Embracing Change: Continual Learning in Deep Neural Networks](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66132030219-9) by Hadsell, R., Rao, D., Rusu, A. A., & Pascanu, R. Trends in cognitive sciences, 24(12)(2020)
* [The Present and Future of Continual Learning](https://ieeexplore.ieee.org/abstract/document/9289549) by Bae, H., Song, S., & Park, J. In 2020 International Conference on Information and Communication Technology Convergence (ICTC)
* [A Continual Learning Survey: Defying Forgetting in Classification Tasks](https://ieeexplore.ieee.org/abstract/document/9349197) by De Lange, Matthias, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. IEEE transactions on pattern analysis and machine intelligence, 44(7)(2021)
* [Recent Advances of Continual Learning in Computer Vision: An Overview](https://arxiv.org/abs/2109.11369) by Qu, H., Rahmani, H., Xu, L., Williams, B., & Liu, J.arXiv preprint(2021) 
* [Continual Learning of Natural Language Processing Tasks: A Survey](https://arxiv.org/abs/2211.12701) by Ke, Z., & Liu, B. arXiv preprint(2022)
* [How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition](https://arxiv.org/abs/2207.07730) by Mendez, J. A., & Eaton, E. arXiv preprint(2022)
* [A Comprehensive Survey of Continual Learning: Theory, Method and Application](https://ieeexplore.ieee.org/abstract/document/10444954) by Wang, L., Zhang, X., Su, H., & Zhu, J. IEEE Transactions on Pattern Analysis and Machine Intelligence.(2024) 

<a name='apps'></a>
## Continual Learning Applications
* [Neural Topic Modeling with Continual Lifelong Learning](https://proceedings.mlr.press/v119/gupta20a.html) by Gupta, P., Chaudhary, Y., Runkler, T., & Schuetze, H. In International Conference on Machine Learning(2020).
* [Continual Learning Of Predictive Models In Video Sequences Via Variational Autoencoders](https://ieeexplore.ieee.org/abstract/document/9190980) by Campo, D., Slavic, G., Baydoun, M., Marcenaro, L., & Regazzoni, C. In 2020 IEEE International Conference on Image Processing.
* [Continual Learning for Unsupervised Anomaly Detection in Continuous Auditing of Financial Accounting Data
](https://arxiv.org/abs/2112.13215) by Hemati, H., Schreyer, M., & Borth, D.arXiv preprint(2021)
* [Incremental Learning for End-to-End Automatic Speech Recognition](https://ieeexplore.ieee.org/abstract/document/9687910) by Fu, L., Li, X., Zi, L., Zhang, Z., Wu, Y., He, X., & Zhou, B. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop.
* [The Traffic Flow Prediction Method Using the Incremental Learning-Based CNN-LTSM Model: The Solution of Mobile Application](https://onlinelibrary.wiley.com/doi/full/10.1155/2021/5579451) by Shao, Y., Zhao, Y., Yu, F., Zhu, H., & Fang, J. .Mobile Information Systems, 2021

<a name='frameworks'></a>
## Continual Learning Frameworks
* [Dark Experience for General Continual Learning: a Strong, Simple Baseline](https://proceedings.neurips.cc/paper/2020/hash/b704ea2c39778f07c617f6b7ce480e9e-Abstract.html) by Buzzega, P., Boschini, M., Porrello, A., Abati, D., & Calderara, S. Advances in neural information processing systems, 33(2020)
* [Class-Incremental Continual Learning Into the eXtended DER-Verse](https://ieeexplore.ieee.org/abstract/document/9891836) by Boschini, M., Bonicelli, L., Buzzega, P., Porrello, A., & Calderara, S.IEEE transactions on pattern analysis and machine intelligence, 45(5)(2022)

<a name='pi'></a>
## Parameter Isolation Methods
### Dynamic Architectures
* [A self-organising network that grows when required](https://www.sciencedirect.com/science/article/abs/pii/S0893608002000783) by Marsland, S., Shapiro, J., & Nehmzow, U. Neural networks, 15(8-9)(2002)
* [Knowledge Transfer in Deep Block-Modular Neural Networks](https://link.springer.com/chapter/10.1007/978-3-319-22979-9_27) by Terekhov, A. V., Montone, G., & O’Regan, J. K. In Biomimetic and Biohybrid Systems: 4th International Conference, Living Machines 2015
* [Progressive Neural Networks](https://arxiv.org/abs/1606.04671) by Rusu, Andrei A., Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. arXiv preprint(2016)  
* [Expert Gate: Lifelong Learning With a Network of Experts](https://openaccess.thecvf.com/content_cvpr_2017/html/Aljundi_Expert_Gate_Lifelong_CVPR_2017_paper.html) by Aljundi, R., Chakravarty, P., & Tuytelaars, T.  In Proceedings of the IEEE conference on computer vision and pattern recognition(2017)
* [Lifelong Learning with Dynamically Expandable Networks](https://arxiv.org/abs/1708.01547) by Yoon, J., Yang, E., Lee, J., & Hwang, S. J.arXiv preprint(2017)
* [Reinforced Continual Learning](https://proceedings.neurips.cc/paper/2018/hash/cee631121c2ec9232f3a2f028ad5c89b-Abstract.html) by Xu, J., & Zhu, Z. Advances in neural information processing systems, 31.(2018)
* [Incremental Learning Through Deep Adaptation](https://ieeexplore.ieee.org/abstract/document/8554156) by Rosenfeld, A., & Tsotsos, J. K. IEEE transactions on pattern analysis and machine intelligence, 42(3)(2018)  
* [Compacting, Picking and Growing for Unforgetting Continual Learning](https://proceedings.neurips.cc/paper/2019/hash/3b220b436e5f3d917a1e649a0dc0281c-Abstract.html) by Hung, C. Y., Tu, C. H., Wu, C. E., Chen, C. H., Chan, Y. M., & Chen, C. S.. Advances in neural information processing systems, 32.(2019)
* [Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning](https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html) by Ostapenko, O., Puscas, M., Klein, T., Jahnichen, P., & Nabi, M.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition(2019)
* [Efficient Continual Learning with Modular Networks and Task-Driven Priors](https://arxiv.org/abs/2012.12631) by Veniat, T., Denoyer, L., & Ranzato, M. A. arXiv preprint(2020)  
* [BNS: Building Network Structures Dynamically for Continual Learning](https://proceedings.neurips.cc/paper_files/paper/2021/hash/ac64504cc249b070772848642cffe6ff-Abstract.html) by Qin, Q., Hu, W., Peng, H., Zhao, D., & Liu, B. Advances in Neural Information Processing Systems, 34,(2021)
* [Bayesian Structural Adaptation for Continual Learning](https://proceedings.mlr.press/v139/kumar21a.html) by Kumar, A., Chatterjee, S., & Rai, P. In International Conference on Machine Learning(2021)
* [Continual Learning via Local Module Composition](https://proceedings.neurips.cc/paper/2021/hash/fe5e7cb609bdbe6d62449d61849c38b0-Abstract.html) by Ostapenko, O., Rodriguez, P., Caccia, M., & Charlin, L. Advances in Neural Information Processing Systems, 34(2021)
* [Model Zoo: A Growing "Brain" That Learns Continually](https://arxiv.org/abs/2106.03027) by Ramesh, R., & Chaudhari, P. arXiv preprint(2021)
* [CoSCL: Cooperation of Small Continual Learners is Stronger Than a Big One](https://link.springer.com/chapter/10.1007/978-3-031-19809-0_15) by Wang, L., Zhang, X., Li, Q., Zhu, J., & Zhong, Y. In European Conference on Computer Vision(2022)

### Fixed Networks
* [PathNet: Evolution Channels Gradient Descent in Super Neural Networks](https://arxiv.org/abs/1701.08734) by Fernando, Chrisantha, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra. arXiv preprint(2017)
* [PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning](https://openaccess.thecvf.com/content_cvpr_2018/html/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.html) by Mallya, A., & Lazebnik, S. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition(2018)
* [Overcoming Catastrophic Forgetting with Hard Attention to the Task](https://proceedings.mlr.press/v80/serra18a) by Serra, J., Suris, D., Miron, M., & Karatzoglou, A. In International conference on machine learning(2018)
* [Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights](https://openaccess.thecvf.com/content_ECCV_2018/html/Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper.html) by Mallya, A., Davis, D., & Lazebnik, S. In Proceedings of the European conference on computer vision(2018) 
* [Continual Learning via Neural Pruning](https://arxiv.org/abs/1903.04476) by Golkar, S., Kagan, M., & Cho, K. arXiv preprint(2019)
* [Uncertainty-based Continual Learning with Adaptive Regularization](https://proceedings.neurips.cc/paper/2019/hash/2c3ddf4bf13852db711dd1901fb517fa-Abstract.html) by Ahn, H., Cha, S., Lee, D., & Moon, T. Advances in neural information processing systems, 32(2019)
* [Continual Learning with Node-Importance based Adaptive Group Sparse Regularization](https://proceedings.neurips.cc/paper/2020/hash/258be18e31c8188555c2ff05b4d542c3-Abstract.html) by Jung, S., Ahn, H., Cha, S., & Moon, T. Advances in neural information processing systems, 33(2020)
* [Supermasks in Superposition](https://proceedings.neurips.cc/paper/2020/hash/ad1f8bb9b51f023cdc80cf94bb615aa9-Abstract.html) by Wortsman, M., Ramanujan, V., Liu, R., Kembhavi, A., Rastegari, M., Yosinski, J., & Farhadi, A.  Advances in Neural Information Processing Systems, 33(2020)
* [Helpful or Harmful: Inter-task Association in Continual Learning](https://link.springer.com/chapter/10.1007/978-3-031-20083-0_31) by in, H., & Kim, E.  In European Conference on Computer Vision(2022)
* [Meta-Attention for ViT-Backed Continual Learning](https://openaccess.thecvf.com/content/CVPR2022/html/Xue_Meta-Attention_for_ViT-Backed_Continual_Learning_CVPR_2022_paper.html) by Xue, M., Zhang, H., Song, J., & Song, M. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(2022)
* [NISPA: Neuro-Inspired Stability-Plasticity Adaptation for Continual Learning in Sparse Networks](https://arxiv.org/abs/2206.09117) by Gurbuz, M. B., & Dovrolis, C. arXiv preprint(2022)
* [Forget-free Continual Learning with Winning Subnetworks](https://proceedings.mlr.press/v162/kang22b.html) by Kang, H., Mina, R. J. L., Madjid, S. R. H., Yoon, J., Hasegawa-Johnson, M., Hwang, S. J., & Yoo, C. D. In International Conference on Machine Learning(2022)

### Model Decomposition
* [Scalable and Order-robust Continual Learning with Additive Parameter Decomposition](https://arxiv.org/abs/1902.09432) by Yoon, J., Kim, S., Yang, E., & Hwang, S. J. arXiv preprint(2019)  
* [Adversarial Continual Learning](https://link.springer.com/chapter/10.1007/978-3-030-58621-8_23) by Ebrahimi, S., Meier, F., Calandra, R., Darrell, T., & Rohrbach, M. In Computer Vision–ECCV 2020
* [Conditional Channel Gated Networks for Task-Aware Continual Learning](https://openaccess.thecvf.com/content_CVPR_2020/html/Abati_Conditional_Channel_Gated_Networks_for_Task-Aware_Continual_Learning_CVPR_2020_paper.html) by Abati, D., Tomczak, J., Blankevoort, T., Calderara, S., Cucchiara, R., & Bejnordi, B. E. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition(2020)
* [Calibrating CNNs for Lifelong Learning](https://proceedings.neurips.cc/paper/2020/hash/b3b43aeeacb258365cc69cdaf42a68af-Abstract.html) by Singh, P., Verma, V. K., Mazumder, P., Carin, L., & Rai, P.  Advances in Neural Information Processing Systems, 33(2020)
* [Reparameterizing Convolutions for Incremental Multi-Task Learning Without Task Interference](https://link.springer.com/chapter/10.1007/978-3-030-58565-5_41) by Kanakis, M., Bruggemann, D., Saha, S., Georgoulis, S., Obukhov, A., & Van Gool, L. In Computer Vision–ECCV 2020
* [Generalized Variational Continual Learning](https://arxiv.org/abs/2011.12328) by Loo, N., Swaroop, S., & Turner, R. E. arXiv preprint(2020)
* [Continual Learning with Filter Atom Swapping](https://openreview.net/forum?id=metRpM4Zrcb) by Miao, Z., Wang, Z., Chen, W., & Qiu, Q. In International Conference on Learning Representations(2021).
* [Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors](https://proceedings.mlr.press/v130/mehta21a.html) by Mehta, N., Liang, K., Verma, V. K., & Carin, L.In International Conference on Artificial Intelligence and Statistics(2021)
* [Optimizing Reusable Knowledge for Continual Learning via Metalearning](https://proceedings.neurips.cc/paper/2021/hash/761e6675f9e54673cc778e7fdb2823d2-Abstract.html) by Hurtado, J., Raymond, A., & Soto, A. Advances in Neural Information Processing Systems, 34(2021)
* [Incremental Learning via Rate Reduction](https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Incremental_Learning_via_Rate_Reduction_CVPR_2021_paper.html) by Wu, Z., Baek, C., You, C., & Ma, Y. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition(2021)
* [DyTox: Transformers for Continual Learning With DYnamic TOken eXpansion](https://openaccess.thecvf.com/content/CVPR2022/html/Douillard_DyTox_Transformers_for_Continual_Learning_With_DYnamic_TOken_eXpansion_CVPR_2022_paper.html) by Douillard, A., Ramé, A., Couairon, G., & Cord, M. n Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(2022)






